{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start...\n",
      "Column names are \n",
      "59205591\n",
      "done...\n"
     ]
    }
   ],
   "source": [
    "#################Reading the dataset and considering the items that have more than one click for a unique query #################\n",
    "#################################################################################################################################\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "count = 0\n",
    "dataset = defaultdict(list)\n",
    "print (\"start...\")\n",
    "with open('st_oms_signals_2020_apr_sep.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "#         print(row)\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        else:\n",
    "            #if the number of clicks is more than 2 for a specific item\n",
    "            if row[-1] >= '2':\n",
    "                count += 1\n",
    "                dataset[row[0]].append(row[1])\n",
    "                line_count += 1\n",
    "\n",
    "              \n",
    "\n",
    "print(count)\n",
    "# print(set(dataset.keys()))\n",
    "print('done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data....\n",
      "done...\n",
      "4781036\n",
      "done....\n"
     ]
    }
   ],
   "source": [
    "################# product detail dataset and create \"taxonomy trajectory\", \"product_category(leaf nodes)\", and item2prodcategory #################\n",
    "#################################################################################################################################\n",
    "\n",
    "print('reading data....')\n",
    "product_details = pd.read_csv('catalog_data_20210125.csv')\n",
    "print('done...')\n",
    "dict_omsid_to_disc = {}\n",
    "dict_prodcategory_to_trajectory = {}\n",
    "dict_item_to_pd = {}\n",
    "product_categories = {}\n",
    "is_in = set()\n",
    "items_all = set()\n",
    "\n",
    "\n",
    "counter = 0\n",
    "for i, row in enumerate(product_details.iterrows()):\n",
    "#     if i > 100:\n",
    "#         break\n",
    "\n",
    "    item_id = str(row[1][0])    \n",
    "    taxonomy = str(row[1][4]).lower()\n",
    "    collection = str(row[1][5]).lower()\n",
    "    title = str(row[1][1]).lower()\n",
    "    brand = str(row[1][2]).lower()\n",
    "    color = str(row[1][-1]).lower()\n",
    "\n",
    "    \n",
    "    dict_omsid_to_disc[item_id] = [title,' '.join(str(taxonomy).split('>')), ' '.join(str(collection).split('/')), brand+ \" </mid> \"+ color]\n",
    "#     print(dict_omsid_to_disc[item_id])\n",
    "\n",
    "\n",
    "    if '>' in str(taxonomy):\n",
    "        prod_cat = taxonomy.split('>')[-1]\n",
    "        dict_item_to_pd[str(item_id)] = prod_cat\n",
    "\n",
    "        if prod_cat in is_in:\n",
    "            continue\n",
    "        else:\n",
    "            is_in.add(prod_cat)\n",
    "            product_categories[prod_cat] = counter\n",
    "            counter += 1\n",
    "            dict_prodcategory_to_trajectory[prod_cat] = taxonomy.split('>')\n",
    "\n",
    "            \n",
    "    \n",
    "print(len(dict_omsid_to_disc.keys()))    \n",
    "\n",
    "with open('dict_prodcategory_to_trajectory.json', 'w') as out_file:\n",
    "    json.dump(dict_prodcategory_to_trajectory, out_file)\n",
    "out_file.close()\n",
    "            \n",
    "            \n",
    "with open('product_categories.json', 'w') as out_file:\n",
    "    json.dump(product_categories, out_file)\n",
    "out_file.close()\n",
    "\n",
    "with open('dict_item_to_productcategory.json', 'w') as out_file:\n",
    "    json.dump(dict_item_to_pd, out_file)\n",
    "out_file.close()\n",
    "\n",
    "print(\"done....\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names are \n",
      "1413429\n",
      "done...\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "dataset_impr = {}\n",
    "with open('st_oms_signals_impr_2020_apr_sep.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "#         if line_count > 1000:\n",
    "#             break\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        else:\n",
    "            count += 1\n",
    "#             print (row)\n",
    "            \n",
    "            vals = row[1].split(\"|\")\n",
    "            new_val = []\n",
    "            for i, v in enumerate(vals):\n",
    "                if len(v) > 0 and i < 10:\n",
    "                    new_val.append(v)\n",
    "                    \n",
    "            if len(new_val) >= 3:\n",
    "#                 print(new_val)\n",
    "                dataset_impr[row[0]] = new_val\n",
    "                line_count += 1\n",
    "\n",
    "# print(dataset_impr)             \n",
    "print (line_count)\n",
    "print('done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_omsid_to_disc 4781036\n",
      "start....\n",
      "738523\n",
      "1398259\n",
      "done....\n"
     ]
    }
   ],
   "source": [
    "################################## Creating the index and value files for the labels ##################################\n",
    "#################################################################################################################################\n",
    "print(\"dict_omsid_to_disc\", len(dict_omsid_to_disc.keys()))\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "def create_value_index(labels):\n",
    "    # mapping the last label into the (value , index) pairs\n",
    "    # it has only one so session gets[0]\n",
    "    lst_labels = labels\n",
    "\n",
    "    values_ = np.zeros(10)\n",
    "    indexes_ = np.zeros(10, dtype=int)\n",
    "    rnd_labels = set()\n",
    "\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            if lst_labels[i] not in rnd_labels:\n",
    "                rnd_labels.add(lst_labels[i])\n",
    "                values_[i] = 1\n",
    "                indexes_[i] = lst_labels[i]\n",
    "\n",
    "            else:\n",
    "                for ii in range(10):\n",
    "                    ind_rnd = random.randint(0, len(product_categories) - 1)\n",
    "                    if ind_rnd not in rnd_labels:\n",
    "                        rnd_labels.add(ind_rnd)\n",
    "                        break\n",
    "                indexes_[i] = ind_rnd\n",
    "                values_[i] = 1e-7\n",
    "\n",
    "        except:\n",
    "            for ii in range(10):\n",
    "                ind_rnd = random.randint(0, len(product_categories) - 1)\n",
    "                if ind_rnd not in rnd_labels:\n",
    "                    rnd_labels.add(ind_rnd)\n",
    "                    break\n",
    "            indexes_[i] = ind_rnd\n",
    "            values_[i] = 1e-7\n",
    "\n",
    "\n",
    "    new_indexes_ii = sorted(range(len(indexes_)), key=lambda k: indexes_[k])\n",
    "    new_values_ = [values_[i] for i in new_indexes_ii]\n",
    "    new_indexes_ = [indexes_[i] for i in new_indexes_ii]\n",
    "    \n",
    "    indexes = defaultdict(list)\n",
    "    values = defaultdict(list)\n",
    "    indexes['ind'].extend(new_indexes_)\n",
    "    values['val'].extend(new_values_)\n",
    "    final_labels = {'indexes': new_indexes_, 'values': new_values_}\n",
    "    return final_labels\n",
    "\n",
    "\n",
    "################################################################\n",
    "################################################################\n",
    "################################################################\n",
    "print(\"start....\")\n",
    "new_dataset = []\n",
    "max_val = []\n",
    "\n",
    "queries1 = set(dataset_impr.keys())\n",
    "queries2 = set(dataset.keys())\n",
    "\n",
    "print (len(queries2-queries1))\n",
    "\n",
    "for i,query in enumerate(dataset):\n",
    "#     if i > 1000:\n",
    "#         break\n",
    "    new_sample = {}\n",
    "    new_sample['query'] = query\n",
    "\n",
    "    categories = Counter()\n",
    "    for item in dataset[query]:\n",
    "        try:\n",
    "            categories[product_categories[dict_item_to_pd[item]]] += 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        imprs = dataset_impr[query]\n",
    "    except:\n",
    "        imprs = []\n",
    "    \n",
    "#     print (imprs)\n",
    "    if len(imprs) > 0:\n",
    "        short_disc = []\n",
    "        for im in imprs:\n",
    "            if len(im) > 0:\n",
    "                try:\n",
    "                    omsid_to_disc = dict_omsid_to_disc[str(im)]\n",
    "                    short_disc.append(omsid_to_disc)\n",
    "                except:\n",
    "                    pass\n",
    "#                 print(omsid_to_disc)\n",
    "\n",
    "        if len(categories) > 0:\n",
    "            prod_cat = dict((k, v) for k, v in categories.items() if v >=0).keys()\n",
    "            final_labels = create_value_index(list(prod_cat))\n",
    "            new_sample['label'] = final_labels\n",
    "            new_sample['short_disc'] = short_disc\n",
    "            new_dataset.append(new_sample)\n",
    "\n",
    "\n",
    "    \n",
    "print (len(new_dataset))\n",
    "print(\"done....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start...\n",
      "699130\n",
      "done....\n"
     ]
    }
   ],
   "source": [
    "################################## Shuffle and extracting 0.95 percent for training ##################################\n",
    "######################################################################################################################\n",
    "\n",
    "from numpyencoder import NumpyEncoder\n",
    "\n",
    "random.shuffle(new_dataset)\n",
    "print('start...')\n",
    "file_ = open(\"train_dataset_10.txt\", 'w')\n",
    "count = 0\n",
    "for i, sample in enumerate(new_dataset):\n",
    "    if i >= 0.5 * len(new_dataset):\n",
    "        break\n",
    "#     print (sample)\n",
    "    file_.write(json.dumps(sample, cls= NumpyEncoder))\n",
    "    file_.write('\\n')\n",
    "    count += 1\n",
    "file_.close()\n",
    "\n",
    "print (count)\n",
    "print(\"done....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start...\n",
      "279651\n",
      "done....\n"
     ]
    }
   ],
   "source": [
    "################################## Shuffle and extracting 0.05 percent for validation ##################################\n",
    "########################################################################################################################\n",
    "\n",
    "from numpyencoder import NumpyEncoder\n",
    "\n",
    "print('start...')\n",
    "file_ = open(\"validation_set_10.txt\", 'w')\n",
    "count = 0\n",
    "for i, sample in enumerate(new_dataset):\n",
    "    if i > len(new_dataset) * 0.8:\n",
    "        sample[\"Bucket\"] = ['1']\n",
    "        file_.write(json.dumps(sample, cls= NumpyEncoder))\n",
    "        file_.write('\\n')\n",
    "        count += 1\n",
    "file_.close()\n",
    "\n",
    "print (count)\n",
    "print(\"done....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start...\n",
      "done....\n"
     ]
    }
   ],
   "source": [
    "print('start...')\n",
    "file_ = open(\"validation_set_10.txt\", 'r').read().split(\"\\n\")\n",
    "# print (file_[0:100])\n",
    "count = 0\n",
    "data_query = []\n",
    "for i, sample in enumerate(file_):\n",
    "    if len(sample) > 0: \n",
    "        line = json.loads(sample)\n",
    "        data_query.append([line['query']])\n",
    "\n",
    "\n",
    "import csv  \n",
    "    \n",
    "# field names  \n",
    "fields = ['query']  \n",
    "    \n",
    "# name of csv file  \n",
    "filename = \"test_queries.csv\"\n",
    "    \n",
    "# writing to csv file  \n",
    "with open(filename, 'w') as csvfile:  \n",
    "    # creating a csv writer object  \n",
    "    csvwriter = csv.writer(csvfile)  \n",
    "        \n",
    "    # writing the fields  \n",
    "    csvwriter.writerow(fields)  \n",
    "        \n",
    "    # writing the data rows  \n",
    "    csvwriter.writerows(data_query) \n",
    "    \n",
    "print(\"done....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start...\n",
      "139321\n",
      "done....\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from numpyencoder import NumpyEncoder\n",
    "\n",
    "\n",
    "from csv import reader\n",
    "# open file in read mode\n",
    "buckets = set(['Head', 'Mid', 'Tail'])\n",
    "\n",
    "volume = {}\n",
    "with open('head_mid_tail.csv', 'r') as read_obj:\n",
    "    csv_reader = csv.reader(read_obj)\n",
    "    for i, row in enumerate(csv_reader):\n",
    "        if i > 0:\n",
    "            if row[8] in buckets:\n",
    "                volume[row[1]] = row[8]\n",
    "            \n",
    "\n",
    "# print(len())\n",
    "\n",
    "print('start...')\n",
    "file_ = open(\"validation_set_bucket_10.txt\", 'w')\n",
    "count = 0\n",
    "for i, sample in enumerate(new_dataset):\n",
    "    try:\n",
    "        if i > len(new_dataset) * 0.5:\n",
    "\n",
    "            sample[\"Bucket\"] = [volume[sample['query']]]\n",
    "            file_.write(json.dumps(sample, cls= NumpyEncoder))\n",
    "            file_.write('\\n')\n",
    "            count += 1\n",
    "    except:\n",
    "        pass\n",
    "file_.close()\n",
    "\n",
    "print (count)\n",
    "print(\"done....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['num_attr_f', 'num_dim_f', 'upc_f', 'sku_f', 'modelNumber_f', 'brand_f', 'product', 'color-material'])\n",
      "start...\n",
      "9276\n",
      "done....\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from numpyencoder import NumpyEncoder\n",
    "\n",
    "\n",
    "from csv import reader\n",
    "# open file in read mode\n",
    "\n",
    "buckets_attrs = {\"num_attr_f\" :\"numerical\",  \"num_dim_f\":\"numerical\", \"upc_f\" : \"ID\", \"sku_f\":\"ID\", \"modelNumber_f\":\"ID\"\n",
    "                 ,\"brand_f\":\"brand\", \"product\":\"product\", \"color-material\":\"color\"}\n",
    "\n",
    "\n",
    "print(buckets_attrs.keys())\n",
    "volume = {}\n",
    "with open('df_seg.csv', 'r') as read_obj:\n",
    "    csv_reader = csv.reader(read_obj)\n",
    "    for i, row in enumerate(csv_reader):\n",
    "        if i > 0:\n",
    "            volume[row[2]] = row[1]\n",
    "            \n",
    "#         if i == 10:\n",
    "#             break\n",
    "# print(volume)\n",
    "\n",
    "print('start...')\n",
    "file_ = open(\"validation_set_bucket_segments.txt\", 'w')\n",
    "count = 0\n",
    "for i, sample in enumerate(new_dataset):\n",
    "    try:\n",
    "        if i > len(new_dataset) * 0.5:\n",
    "\n",
    "            attrs = volume[sample['query']].split(';')\n",
    "            attrs_bucket = set()\n",
    "            for att in attrs:\n",
    "#                 print (att)\n",
    "                if att in buckets_attrs.keys():\n",
    "                    attrs_bucket.add(buckets_attrs[att])\n",
    "            \n",
    "            if len(list(attrs_bucket)) > 0:\n",
    "                sample[\"Bucket\"] = list(attrs_bucket)\n",
    "                file_.write(json.dumps(sample, cls= NumpyEncoder))\n",
    "                file_.write('\\n')\n",
    "                count += 1\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "file_.close()\n",
    "\n",
    "print (count)\n",
    "print(\"done....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
